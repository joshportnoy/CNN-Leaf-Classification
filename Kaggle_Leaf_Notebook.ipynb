{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classification\n",
    "\n",
    "The Leaf Classification Kaggle Competition offers an educational image training set of 99 leaves. The training data is comprised of an image set with 16 samples of each of 99 species of leaves, as well as 3 64-attribute vectors per leaf sample. The image data is first loaded and resized to 64x64 to ensure all images are of the same size for inputting into the convolutional neural network (CNN), and class labels are encoded for ease of data processing. The proposed CNN architechture is comprised of a convolutional input layer with 5x5 kernel and 16 filters, followed by a 2x2 MaxPooling layer and a 50% dropout to regularize the model and reduce overfitting. Afterwards, two convolutional layers with 32 filters each and 3x3 kernels are used in place of a 5x5 kernel to reduce parameters and computational requirements. Afterwards, another MaxPooling layer is added to further reduce dimensionality as well as the likelihood of overfitting. The data is then passed through a flattening layer before a fully connected dense layer and the subsequent output layer with a softmax activation function. \n",
    "\n",
    "The first model is tested with a batch size of 32, 10 epochs, and samples per epoch equal to the size of the training data (768). While this model resulted in a 98% accuracy on the training set, the validation set only reached 72% accuracy. Furthermore, a graphical comparison of loss and accuracy between the training and validation sets indicates overfitting. The lack of decrease in validation loss with subsequent epochs suggests this may be due to an overly high number of samples per epoch. To address this, a second model is proposed with samples per epoch equal to the size of the training data divided by batch size (24). The loss and accuracy functions for this second model confirm that a lower number of samples per epoch reduces overfitting, with validation and training curves showing little deviation from one another. However, the low number of epochs does not appear to be enough for the loss functions level off and the model resulted in a low accuracy of only 62% for training data and 53% for the validation. The third model explores this hypothesis by increasing the number of epochs from 10 to 30, while keeping all other parameters constant. Increasing the number of epochs succeeds in increasing model accuracy for both training and validation sets to 83% and 66% respectively. While this is better performing than the previous model, it does not yet surpass the first proposed model. For further hyperparameter tuning, a fourth model is implemented to balance the benefits of a lower number of samples found in model 2, with the higher accuracy resulting from more epochs. Model 4 is deployed with 15 epochs and 278 samples per epoch, resulting in a 97% training set accuracy and an 80% accuracy on the validation set. This final model was submitted to the Kaggle competition and received a score of  1.69522 based on the logloss scoring function. To further improve this modelâ€™s performance, additional action against overfitting is suggested. While regularization parameters where explored with minimal effect, increasing the training set through image rotation and translation may prove to be very beneficial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:53:15.919640Z",
     "iopub.status.busy": "2022-03-02T01:53:15.919372Z",
     "iopub.status.idle": "2022-03-02T01:53:22.615663Z",
     "shell.execute_reply": "2022-03-02T01:53:22.614895Z",
     "shell.execute_reply.started": "2022-03-02T01:53:15.919602Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "from cv2 import imread, resize\n",
    "import zipfile\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler, LabelEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score,accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize, rotate, SimilarityTransform, warp\n",
    "from skimage.filters import sobel\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import max_norm, unit_norm\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, InputLayer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:53:22.617598Z",
     "iopub.status.busy": "2022-03-02T01:53:22.617104Z",
     "iopub.status.idle": "2022-03-02T01:53:24.714411Z",
     "shell.execute_reply": "2022-03-02T01:53:24.713654Z",
     "shell.execute_reply.started": "2022-03-02T01:53:22.617560Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:53:24.716439Z",
     "iopub.status.busy": "2022-03-02T01:53:24.715934Z",
     "iopub.status.idle": "2022-03-02T01:53:24.809106Z",
     "shell.execute_reply": "2022-03-02T01:53:24.808158Z",
     "shell.execute_reply.started": "2022-03-02T01:53:24.716399Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/leaf-classification/train.csv.zip', usecols=['id', 'species'])\n",
    "mtrain = train.shape[0]\n",
    "test = pd.read_csv('../input/leaf-classification/test.csv.zip', usecols = [0])\n",
    "mtest = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:53:24.811896Z",
     "iopub.status.busy": "2022-03-02T01:53:24.811385Z",
     "iopub.status.idle": "2022-03-02T01:54:18.738132Z",
     "shell.execute_reply": "2022-03-02T01:54:18.737369Z",
     "shell.execute_reply.started": "2022-03-02T01:53:24.811857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resize and read images\n",
    "# Fill image to same size\n",
    "print('Loading and resizing images...')# % len(image_paths))\n",
    "img_rows, img_cols = 64, 64 #96,96 #40, 40\n",
    "output_shape = (img_rows, img_cols)\n",
    "\n",
    "train_images = np.zeros((mtrain, img_rows, img_cols))\n",
    "for i in range(mtrain):\n",
    "    image = imread('../input/leaf-classification/images.zip/images/'+str(train.id.iloc[i])+'.jpg')\n",
    "    rimage = resize(image, output_shape=output_shape)\n",
    "    train_images[i] = sobel(rimage)\n",
    "    \n",
    "test_images = np.zeros((mtest, img_rows, img_cols))\n",
    "for i in range(mtest):\n",
    "    image = imread('../input/leaf-classification/images.zip/images/'+str(test.id.iloc[i])+'.jpg')\n",
    "    rimage = resize(image, output_shape=output_shape)\n",
    "    test_images[i] = sobel(rimage)\n",
    "\n",
    "print('Train images shape: {}'.format(train_images.shape)) # 990\n",
    "print('Test images shape: {}'.format(test_images.shape)) # 594"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:54:18.739802Z",
     "iopub.status.busy": "2022-03-02T01:54:18.739424Z",
     "iopub.status.idle": "2022-03-02T01:54:18.747360Z",
     "shell.execute_reply": "2022-03-02T01:54:18.746583Z",
     "shell.execute_reply.started": "2022-03-02T01:54:18.739766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target\n",
    "le = LabelEncoder()\n",
    "target = le.fit_transform(train.species)\n",
    "print(\"Target shape: {}\".format(target.shape))\n",
    "print(np.unique(target).size)\n",
    "print(target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:54:18.749171Z",
     "iopub.status.busy": "2022-03-02T01:54:18.748717Z",
     "iopub.status.idle": "2022-03-02T01:54:18.756311Z",
     "shell.execute_reply": "2022-03-02T01:54:18.755591Z",
     "shell.execute_reply.started": "2022-03-02T01:54:18.749129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale training data\n",
    "#scaler = StandardScaler().fit(train_images)\n",
    "#train_images = scaler.transform(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T01:54:18.762518Z",
     "iopub.status.busy": "2022-03-02T01:54:18.757655Z",
     "iopub.status.idle": "2022-03-02T01:54:18.785198Z",
     "shell.execute_reply": "2022-03-02T01:54:18.783980Z",
     "shell.execute_reply.started": "2022-03-02T01:54:18.758169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Image batch generator\n",
    "def imageGenerator(X, y, batch_size):\n",
    "    img_rows, img_cols = X.shape[1], X.shape[2]\n",
    "    resc = 0.02\n",
    "    rot = 5\n",
    "    transl = 0.01*img_rows\n",
    "    while 1: # Infinite loop\n",
    "        batchX = np.zeros((batch_size, img_rows, img_cols, 1))\n",
    "        # batch_size random indices over train images\n",
    "        batch_ids = np.random.choice(X.shape[0], batch_size)\n",
    "        for j in range(batch_ids.shape[0]): # Loop over random images\n",
    "            # Rotate around center\n",
    "            imagej = rotate(X[batch_ids[j]], angle =rot*np.random.randn())\n",
    "            # Rescale and translate\n",
    "            tf = SimilarityTransform(scale = 1 + resc*np.random.randn(1,2)[0],\n",
    "                                translation = transl*np.random.randn(1,2)[0]) \n",
    "            batchX[j] = warp(imagej, tf)\n",
    "        #batchX = np.reshape(batchX, (batch_size,-1)) # Flattened images for FNN\n",
    "        #print(batchX.shape, y[batch_ids].shape)\n",
    "        yield (batchX, y[batch_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "Model Parameters:\n",
    "Batch size: 32, epochs: 10, samples per epoch: 768, n classes: 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:54:20.603285Z",
     "iopub.status.busy": "2022-03-01T23:54:20.602594Z",
     "iopub.status.idle": "2022-03-01T23:54:20.642435Z",
     "shell.execute_reply": "2022-03-01T23:54:20.641735Z",
     "shell.execute_reply.started": "2022-03-01T23:54:20.603239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create random train and validation sets out of 20% samples\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(train_images, target, test_size=0.20,\n",
    "                                           stratify=target, random_state=14)\n",
    "\n",
    "print('\\nXtrain, ytrain shapes ' + str((Xtrain.shape, ytrain.shape)))\n",
    "print('Xval, yval shapes ' + str((Xval.shape, yval.shape)))\n",
    "\n",
    "# Reshape data as a 4-dim tensor [number samples, width, height, color channels]\n",
    "print('Reshape as 4-dim tensor (Tensorflow backend)')\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0], img_rows, img_cols, 1) # 792\n",
    "Xval = Xval.reshape(Xval.shape[0], img_rows, img_cols, 1) # 198\n",
    "Xtest = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(Xtrain.shape, Xval.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:54:20.645027Z",
     "iopub.status.busy": "2022-03-01T23:54:20.644560Z",
     "iopub.status.idle": "2022-03-01T23:54:20.652720Z",
     "shell.execute_reply": "2022-03-01T23:54:20.651824Z",
     "shell.execute_reply.started": "2022-03-01T23:54:20.644989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "print('Train convolutional neural network')\n",
    "# Parameters\n",
    "# Batch size: number of training examples in one forward/backward pass. \n",
    "# The higher the batch size, the more memory space you'll need\n",
    "batch_size = 32\n",
    "# Epoch: one forward pass and one backward pass of all the training examples\n",
    "nb_epoch = 10\n",
    "n_extension = 1\n",
    "samples_per_epoch = batch_size*(n_extension*Xtrain.shape[0] // batch_size)\n",
    "#samples_per_epoch = (Xtrain.shape[0] // batch_size)\n",
    "\n",
    "# number of convolutional filters to use (a hidden layer is segmented into feature maps\n",
    "# where each unit in a feature map looks for the same feature but at different positions of the input image)\n",
    "#nb_filters = 64\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "# Number of classes\n",
    "nb_classes = np.unique(ytrain).size\n",
    "#kernel_size = (5, 5)\n",
    "print('Model Parameters:')\n",
    "print('Batch size: %d, epochs: %d, samples per epoch: %d, n classes: %d' % \n",
    "                                (batch_size, nb_epoch,samples_per_epoch, nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:54:20.655648Z",
     "iopub.status.busy": "2022-03-01T23:54:20.654637Z",
     "iopub.status.idle": "2022-03-01T23:54:20.984160Z",
     "shell.execute_reply": "2022-03-01T23:54:20.983403Z",
     "shell.execute_reply.started": "2022-03-01T23:54:20.655616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoder)\n",
    "ytrain = np_utils.to_categorical(ytrain, nb_classes)\n",
    "yval = np_utils.to_categorical(yval, nb_classes)\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "\n",
    "# Add hidden layers\n",
    "\n",
    "# Conv2D layer with 5x5 kernels (local weights) and 16 conv filters \n",
    "# (or feature maps), expects 2d images as inputs\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5),  padding='same', input_shape=input_shape,activation='relu',))\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.5)) # Regularization method, exclude 50% units\n",
    "\n",
    "# Stack two 3x3 conv2D layers (Instead of one 5x5)\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=input_shape,activation='relu',))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=input_shape,activation='relu',))\n",
    "\n",
    "# Pool2D layer, a form of non-linear down-sampling to prevent\n",
    "# overfitting and provide a form of translation invariance\n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Flattenig layer, converts 2D matrix into vectors\n",
    "model.add(Flatten())\n",
    "\n",
    "# Standard fully connected layer with 128 units\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:54:20.986050Z",
     "iopub.status.busy": "2022-03-01T23:54:20.985789Z",
     "iopub.status.idle": "2022-03-01T23:57:08.478424Z",
     "shell.execute_reply": "2022-03-01T23:57:08.477713Z",
     "shell.execute_reply.started": "2022-03-01T23:54:20.986014Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining Keras Convolutional Neural Network...')\n",
    "# Fit model with generator\n",
    "results = model.fit_generator(imageGenerator(Xtrain, ytrain, batch_size), \n",
    "                    steps_per_epoch = samples_per_epoch,\n",
    "                    epochs=nb_epoch, verbose=1, validation_data=(Xval, yval))\n",
    "#model.fit(Xtrain, ytrain, batch_size=batch_size, epochs=nb_epoch,\n",
    "#          verbose=1, validation_data=(Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:08.480446Z",
     "iopub.status.busy": "2022-03-01T23:57:08.480192Z",
     "iopub.status.idle": "2022-03-01T23:57:08.545935Z",
     "shell.execute_reply": "2022-03-01T23:57:08.545220Z",
     "shell.execute_reply.started": "2022-03-01T23:57:08.480409Z"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(Xval, yval, verbose=0)\n",
    "print('Validation loss: %0.5f' % score[0])\n",
    "print('Validation accuracy: %0.2f' % (100*score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:08.549721Z",
     "iopub.status.busy": "2022-03-01T23:57:08.549501Z",
     "iopub.status.idle": "2022-03-01T23:57:08.562636Z",
     "shell.execute_reply": "2022-03-01T23:57:08.561727Z",
     "shell.execute_reply.started": "2022-03-01T23:57:08.549689Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(results.history)\n",
    "\n",
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:08.564491Z",
     "iopub.status.busy": "2022-03-01T23:57:08.564139Z",
     "iopub.status.idle": "2022-03-01T23:57:08.782042Z",
     "shell.execute_reply": "2022-03-01T23:57:08.781350Z",
     "shell.execute_reply.started": "2022-03-01T23:57:08.564426Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ploting Loss Vs Val_Loss\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:08.783734Z",
     "iopub.status.busy": "2022-03-01T23:57:08.783312Z",
     "iopub.status.idle": "2022-03-01T23:57:08.990322Z",
     "shell.execute_reply": "2022-03-01T23:57:08.989681Z",
     "shell.execute_reply.started": "2022-03-01T23:57:08.783695Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting  Accutacy vs Val_accuracy\n",
    "losses[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Epochs and steps per epoch are sufficient to train model to 98% accuracy on training set, however the model exhibits overfitting as clearly indicated in the graph comparison of training and validation loss. While the training set's loss effectively decreases to 0, the validation loss essentially fails to decrease at all here possibly as a result of too many steps per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "\n",
    "Reduced steps_per_epoch to Training_size/batch_size\n",
    "\n",
    "Model Parameters:\n",
    "Batch size: 32, epochs: 10, samples per epoch: 24, n classes: 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:08.991903Z",
     "iopub.status.busy": "2022-03-01T23:57:08.991642Z",
     "iopub.status.idle": "2022-03-01T23:57:09.017093Z",
     "shell.execute_reply": "2022-03-01T23:57:09.016349Z",
     "shell.execute_reply.started": "2022-03-01T23:57:08.991869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create random train and validation sets out of 20% samples\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(train_images, target, test_size=0.20,\n",
    "                                           stratify=target, random_state=14)\n",
    "\n",
    "print('\\nXtrain, ytrain shapes ' + str((Xtrain.shape, ytrain.shape)))\n",
    "print('Xval, yval shapes ' + str((Xval.shape, yval.shape)))\n",
    "\n",
    "# Reshape data as a 4-dim tensor [number samples, width, height, color channels]\n",
    "print('Reshape as 4-dim tensor (Tensorflow backend)')\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0], img_rows, img_cols, 1) # 792\n",
    "Xval = Xval.reshape(Xval.shape[0], img_rows, img_cols, 1) # 198\n",
    "Xtest = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(Xtrain.shape, Xval.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:09.018778Z",
     "iopub.status.busy": "2022-03-01T23:57:09.018504Z",
     "iopub.status.idle": "2022-03-01T23:57:09.027333Z",
     "shell.execute_reply": "2022-03-01T23:57:09.026568Z",
     "shell.execute_reply.started": "2022-03-01T23:57:09.018741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "print('Train convolutional neural network')\n",
    "# Parameters\n",
    "# Batch size: number of training examples in one forward/backward pass. \n",
    "# The higher the batch size, the more memory space you'll need\n",
    "batch_size = 32\n",
    "# Epoch: one forward pass and one backward pass of all the training examples\n",
    "nb_epoch = 10\n",
    "n_extension = 1\n",
    "#samples_per_epoch = batch_size*(n_extension*Xtrain.shape[0] // batch_size)\n",
    "samples_per_epoch = Xtrain.shape[0] // batch_size\n",
    "\n",
    "# number of convolutional filters to use (a hidden layer is segmented into feature maps\n",
    "# where each unit in a feature map looks for the same feature but at different positions of the input image)\n",
    "#nb_filters = 64\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "# Number of classes\n",
    "nb_classes = np.unique(ytrain).size\n",
    "#kernel_size = (5, 5)\n",
    "print('Model Parameters:')\n",
    "print('Batch size: %d, epochs: %d, samples per epoch: %d, n classes: %d' % \n",
    "                                (batch_size, nb_epoch,samples_per_epoch, nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:09.029100Z",
     "iopub.status.busy": "2022-03-01T23:57:09.028616Z",
     "iopub.status.idle": "2022-03-01T23:57:09.098472Z",
     "shell.execute_reply": "2022-03-01T23:57:09.097797Z",
     "shell.execute_reply.started": "2022-03-01T23:57:09.029058Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoder)\n",
    "ytrain = np_utils.to_categorical(ytrain, nb_classes)\n",
    "yval = np_utils.to_categorical(yval, nb_classes)\n",
    "\n",
    "# Create model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add hidden layers\n",
    "\n",
    "# Conv2D layer with 5x5 kernels (local weights) and 16 conv filters \n",
    "# (or feature maps), expects 2d images as inputs\n",
    "model2.add(Conv2D(filters=16, kernel_size=(5,5),kernel_initializer='he_normal',  padding='same', input_shape=input_shape,activation='relu',))\n",
    "model2.add(MaxPooling2D(pool_size=pool_size))\n",
    "model2.add(Dropout(0.5)) # Regularization method, exclude 50% units\n",
    "\n",
    "# Stack two 3x3 conv2D layers (Instead of one 5x5)\n",
    "model2.add(Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_regularizer=regularizers.l2(l=0.01), input_shape=input_shape,activation='relu',))\n",
    "model2.add(Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_regularizer=regularizers.l2(l=0.01), input_shape=input_shape,activation='relu',))\n",
    "\n",
    "# Pool2D layer, a form of non-linear down-sampling to prevent\n",
    "# overfitting and provide a form of translation invariance\n",
    "model2.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Flattenig layer, converts 2D matrix into vectors\n",
    "model2.add(Flatten())\n",
    "\n",
    "# Standard fully connected layer with 128 units\n",
    "model2.add(Dense(128))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Output layer\n",
    "model2.add(Dense(nb_classes))\n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:09.100094Z",
     "iopub.status.busy": "2022-03-01T23:57:09.099841Z",
     "iopub.status.idle": "2022-03-01T23:57:16.101983Z",
     "shell.execute_reply": "2022-03-01T23:57:16.101243Z",
     "shell.execute_reply.started": "2022-03-01T23:57:09.100060Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining Keras Convolutional Neural Network...')\n",
    "# Fit model with generator\n",
    "results2 = model2.fit_generator(imageGenerator(Xtrain, ytrain, batch_size), \n",
    "                    steps_per_epoch = samples_per_epoch,\n",
    "                    epochs=nb_epoch, verbose=1, validation_data=(Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.103737Z",
     "iopub.status.busy": "2022-03-01T23:57:16.103445Z",
     "iopub.status.idle": "2022-03-01T23:57:16.173374Z",
     "shell.execute_reply": "2022-03-01T23:57:16.172687Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.103690Z"
    }
   },
   "outputs": [],
   "source": [
    "score = model2.evaluate(Xval, yval, verbose=0)\n",
    "print('Validation loss: %0.5f' % score[0])\n",
    "print('Validation accuracy: %0.2f' % (100*score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.174886Z",
     "iopub.status.busy": "2022-03-01T23:57:16.174611Z",
     "iopub.status.idle": "2022-03-01T23:57:16.186841Z",
     "shell.execute_reply": "2022-03-01T23:57:16.185830Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.174850Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(results2.history)\n",
    "\n",
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.189590Z",
     "iopub.status.busy": "2022-03-01T23:57:16.188536Z",
     "iopub.status.idle": "2022-03-01T23:57:16.400836Z",
     "shell.execute_reply": "2022-03-01T23:57:16.400123Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.189553Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ploting Loss Vs Val_Loss\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.402472Z",
     "iopub.status.busy": "2022-03-01T23:57:16.402220Z",
     "iopub.status.idle": "2022-03-01T23:57:16.600298Z",
     "shell.execute_reply": "2022-03-01T23:57:16.599528Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.402427Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting  Accutacy vs Val_accuracy\n",
    "losses[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the number of steps per epoch helped to address the overfitting issue as observed in the loss graphs for training and validation sets, however the overall accuracy for training and validation sets are much lower than in the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "\n",
    "Increase number of epochs since last model's loss did not appear to converge yet.\n",
    "\n",
    "Model Parameters:\n",
    "Batch size: 32, epochs: 30, samples per epoch: 24, n classes: 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.601772Z",
     "iopub.status.busy": "2022-03-01T23:57:16.601501Z",
     "iopub.status.idle": "2022-03-01T23:57:16.626061Z",
     "shell.execute_reply": "2022-03-01T23:57:16.625348Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.601737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create random train and validation sets out of 20% samples\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(train_images, target, test_size=0.20,\n",
    "                                           stratify=target, random_state=14)\n",
    "\n",
    "print('\\nXtrain, ytrain shapes ' + str((Xtrain.shape, ytrain.shape)))\n",
    "print('Xval, yval shapes ' + str((Xval.shape, yval.shape)))\n",
    "\n",
    "# Reshape data as a 4-dim tensor [number samples, width, height, color channels]\n",
    "print('Reshape as 4-dim tensor (Tensorflow backend)')\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0], img_rows, img_cols, 1) # 792\n",
    "Xval = Xval.reshape(Xval.shape[0], img_rows, img_cols, 1) # 198\n",
    "Xtest = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(Xtrain.shape, Xval.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.627707Z",
     "iopub.status.busy": "2022-03-01T23:57:16.627430Z",
     "iopub.status.idle": "2022-03-01T23:57:16.634885Z",
     "shell.execute_reply": "2022-03-01T23:57:16.633922Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.627651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "print('Train convolutional neural network')\n",
    "# Parameters\n",
    "# Batch size: number of training examples in one forward/backward pass. \n",
    "# The higher the batch size, the more memory space you'll need\n",
    "batch_size = 32\n",
    "# Epoch: one forward pass and one backward pass of all the training examples\n",
    "nb_epoch = 30\n",
    "n_extension = 1\n",
    "#samples_per_epoch = batch_size*(n_extension*Xtrain.shape[0] // batch_size)\n",
    "samples_per_epoch = Xtrain.shape[0] // batch_size\n",
    "\n",
    "# number of convolutional filters to use (a hidden layer is segmented into feature maps\n",
    "# where each unit in a feature map looks for the same feature but at different positions of the input image)\n",
    "#nb_filters = 64\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "# Number of classes\n",
    "nb_classes = np.unique(ytrain).size\n",
    "#kernel_size = (5, 5)\n",
    "print('Model Parameters:')\n",
    "print('Batch size: %d, epochs: %d, samples per epoch: %d, n classes: %d' % \n",
    "                                (batch_size, nb_epoch,samples_per_epoch, nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.636882Z",
     "iopub.status.busy": "2022-03-01T23:57:16.636589Z",
     "iopub.status.idle": "2022-03-01T23:57:16.706127Z",
     "shell.execute_reply": "2022-03-01T23:57:16.705475Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.636845Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoder)\n",
    "ytrain = np_utils.to_categorical(ytrain, nb_classes)\n",
    "yval = np_utils.to_categorical(yval, nb_classes)\n",
    "\n",
    "# Create model\n",
    "model3 = Sequential()\n",
    "\n",
    "# Add hidden layers\n",
    "\n",
    "# Conv2D layer with 5x5 kernels (local weights) and 16 conv filters \n",
    "# (or feature maps), expects 2d images as inputs\n",
    "model3.add(Conv2D(filters=16, kernel_size=(5,5),  padding='same', input_shape=input_shape,activation='relu',))\n",
    "model3.add(MaxPooling2D(pool_size=pool_size))\n",
    "model3.add(Dropout(0.5)) # Regularization method, exclude 50% units\n",
    "\n",
    "# Stack two 3x3 conv2D layers (Instead of one 5x5)\n",
    "model3.add(Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_constraint=max_norm(3), bias_constraint=max_norm(3), input_shape=input_shape,activation='relu',))\n",
    "model3.add(Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_constraint=max_norm(3), bias_constraint=max_norm(3), input_shape=input_shape,activation='relu',))\n",
    "\n",
    "# Pool2D layer, a form of non-linear down-sampling to prevent\n",
    "# overfitting and provide a form of translation invariance\n",
    "model3.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Flattenig layer, converts 2D matrix into vectors\n",
    "model3.add(Flatten())\n",
    "\n",
    "# Standard fully connected layer with 128 units\n",
    "model3.add(Dense(128))\n",
    "model3.add(Activation('relu'))\n",
    "\n",
    "# Output layer\n",
    "model3.add(Dense(nb_classes))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:16.707648Z",
     "iopub.status.busy": "2022-03-01T23:57:16.707384Z",
     "iopub.status.idle": "2022-03-01T23:57:37.728416Z",
     "shell.execute_reply": "2022-03-01T23:57:37.727503Z",
     "shell.execute_reply.started": "2022-03-01T23:57:16.707614Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\nTraining Keras Convolutional Neural Network...')\n",
    "# Fit model with generator\n",
    "results3 = model3.fit_generator(imageGenerator(Xtrain, ytrain, batch_size), \n",
    "                    steps_per_epoch = samples_per_epoch,\n",
    "                    epochs=nb_epoch, verbose=1, validation_data=(Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:37.730852Z",
     "iopub.status.busy": "2022-03-01T23:57:37.730451Z",
     "iopub.status.idle": "2022-03-01T23:57:37.802757Z",
     "shell.execute_reply": "2022-03-01T23:57:37.802018Z",
     "shell.execute_reply.started": "2022-03-01T23:57:37.730784Z"
    }
   },
   "outputs": [],
   "source": [
    "score = model3.evaluate(Xval, yval, verbose=0)\n",
    "print('Validation loss: %0.5f' % score[0])\n",
    "print('Validation accuracy: %0.2f' % (100*score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:37.804419Z",
     "iopub.status.busy": "2022-03-01T23:57:37.804169Z",
     "iopub.status.idle": "2022-03-01T23:57:37.815727Z",
     "shell.execute_reply": "2022-03-01T23:57:37.814732Z",
     "shell.execute_reply.started": "2022-03-01T23:57:37.804384Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(results3.history)\n",
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:37.817952Z",
     "iopub.status.busy": "2022-03-01T23:57:37.817395Z",
     "iopub.status.idle": "2022-03-01T23:57:38.046530Z",
     "shell.execute_reply": "2022-03-01T23:57:38.045848Z",
     "shell.execute_reply.started": "2022-03-01T23:57:37.817886Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ploting Loss Vs Val_Loss\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-01T23:57:38.048174Z",
     "iopub.status.busy": "2022-03-01T23:57:38.047912Z",
     "iopub.status.idle": "2022-03-01T23:57:38.265261Z",
     "shell.execute_reply": "2022-03-01T23:57:38.264565Z",
     "shell.execute_reply.started": "2022-03-01T23:57:38.048138Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting  Accutacy vs Val_accuracy\n",
    "losses[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing number of epochs successfully increases accuracy score for both training and validation sets. The curves of loss and accuracy for training and validation appear to diverge after about 7 epochs, with the validation set leveling off around 65% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balances between the accuracy and fitting of models 2 and 3.\n",
    "\n",
    "Model Parameters:\n",
    "Batch size: 32, epochs: 15, samples per epoch: 278, n classes: 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:11:24.225500Z",
     "iopub.status.busy": "2022-03-02T00:11:24.225178Z",
     "iopub.status.idle": "2022-03-02T00:11:24.251191Z",
     "shell.execute_reply": "2022-03-02T00:11:24.250357Z",
     "shell.execute_reply.started": "2022-03-02T00:11:24.225467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create random train and validation sets out of 10% samples\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(train_images, target, test_size=0.10,\n",
    "                                           stratify=target, random_state=14)\n",
    "\n",
    "#print('\\nXtrain, ytrain shapes ' + str((Xtrain.shape, ytrain.shape)))\n",
    "#print('Xval, yval shapes ' + str((Xval.shape, yval.shape)))\n",
    "\n",
    "# Reshape data as a 4-dim tensor [number samples, width, height, color channels]\n",
    "print('Reshape as 4-dim tensor (Tensorflow backend)')\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0], img_rows, img_cols, 1) # 792\n",
    "Xval = Xval.reshape(Xval.shape[0], img_rows, img_cols, 1) # 198\n",
    "Xtest = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(Xtrain.shape, Xval.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:11:25.969220Z",
     "iopub.status.busy": "2022-03-02T00:11:25.968640Z",
     "iopub.status.idle": "2022-03-02T00:11:25.977225Z",
     "shell.execute_reply": "2022-03-02T00:11:25.976474Z",
     "shell.execute_reply.started": "2022-03-02T00:11:25.969180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "print('Train convolutional neural network')\n",
    "# Parameters\n",
    "# Batch size: number of training examples in one forward/backward pass. \n",
    "# The higher the batch size, the more memory space you'll need\n",
    "batch_size = 32\n",
    "# Epoch: one forward pass and one backward pass of all the training examples\n",
    "nb_epoch = 15\n",
    "n_extension = 10\n",
    "#samples_per_epoch = batch_size*(n_extension*Xtrain.shape[0] // batch_size)\n",
    "samples_per_epoch = n_extension*Xtrain.shape[0] // batch_size\n",
    "\n",
    "# number of convolutional filters to use (a hidden layer is segmented into feature maps\n",
    "# where each unit in a feature map looks for the same feature but at different positions of the input image)\n",
    "#nb_filters = 64\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "# Number of classes\n",
    "nb_classes = np.unique(ytrain).size\n",
    "#kernel_size = (5, 5)\n",
    "print('Model Parameters:')\n",
    "print('Batch size: %d, epochs: %d, samples per epoch: %d, n classes: %d' % \n",
    "                                (batch_size, nb_epoch,samples_per_epoch, nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:11:27.702391Z",
     "iopub.status.busy": "2022-03-02T00:11:27.701602Z",
     "iopub.status.idle": "2022-03-02T00:11:27.778839Z",
     "shell.execute_reply": "2022-03-02T00:11:27.778089Z",
     "shell.execute_reply.started": "2022-03-02T00:11:27.702349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoder)\n",
    "ytrain = np_utils.to_categorical(ytrain, nb_classes)\n",
    "yval = np_utils.to_categorical(yval, nb_classes)\n",
    "\n",
    "# Create model\n",
    "model4 = Sequential()\n",
    "\n",
    "# Add hidden layers\n",
    "\n",
    "# Conv2D layer with 5x5 kernels (local weights) and 16 conv filters \n",
    "# (or feature maps), expects 2d images as inputs\n",
    "model4.add(Conv2D(filters=16, kernel_size=(5,5),  padding='same', input_shape=input_shape,activation='relu',))\n",
    "model4.add(MaxPooling2D(pool_size=pool_size))\n",
    "model4.add(Dropout(0.5)) # Regularization method, exclude 50% units\n",
    "\n",
    "# Stack two 3x3 conv2D layers (Instead of one 5x5)\n",
    "model4.add(Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_constraint=max_norm(3), bias_constraint=max_norm(3), input_shape=input_shape,activation='relu',))\n",
    "model4.add(Conv2D(filters=32, kernel_size=(3,3), padding='same',kernel_constraint=max_norm(3), bias_constraint=max_norm(3), input_shape=input_shape,activation='relu',))\n",
    "\n",
    "# Pool2D layer, a form of non-linear down-sampling to prevent\n",
    "# overfitting and provide a form of translation invariance\n",
    "model4.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Flattenig layer, converts 2D matrix into vectors\n",
    "model4.add(Flatten())\n",
    "\n",
    "# Standard fully connected layer with 128 units\n",
    "model4.add(Dense(128))\n",
    "model4.add(Activation('relu'))\n",
    "\n",
    "# Output layer\n",
    "model4.add(Dense(nb_classes))\n",
    "model4.add(Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:11:31.865549Z",
     "iopub.status.busy": "2022-03-02T00:11:31.865276Z",
     "iopub.status.idle": "2022-03-02T00:13:05.850605Z",
     "shell.execute_reply": "2022-03-02T00:13:05.849877Z",
     "shell.execute_reply.started": "2022-03-02T00:11:31.865518Z"
    }
   },
   "outputs": [],
   "source": [
    "print('\\nTraining Keras Convolutional Neural Network...')\n",
    "# Fit model with generator\n",
    "results4 = model4.fit_generator(imageGenerator(Xtrain, ytrain, batch_size), \n",
    "                    steps_per_epoch = samples_per_epoch,\n",
    "                    epochs=nb_epoch, verbose=1, validation_data=(Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:13:22.542083Z",
     "iopub.status.busy": "2022-03-02T00:13:22.538855Z",
     "iopub.status.idle": "2022-03-02T00:13:22.646281Z",
     "shell.execute_reply": "2022-03-02T00:13:22.644301Z",
     "shell.execute_reply.started": "2022-03-02T00:13:22.542034Z"
    }
   },
   "outputs": [],
   "source": [
    "score = model4.evaluate(Xval, yval, verbose=0)\n",
    "print('Validation loss: %0.5f' % score[0])\n",
    "print('Validation accuracy: %0.2f' % (100*score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:13:24.899437Z",
     "iopub.status.busy": "2022-03-02T00:13:24.899154Z",
     "iopub.status.idle": "2022-03-02T00:13:24.910802Z",
     "shell.execute_reply": "2022-03-02T00:13:24.909912Z",
     "shell.execute_reply.started": "2022-03-02T00:13:24.899406Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(results4.history)\n",
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:13:33.142198Z",
     "iopub.status.busy": "2022-03-02T00:13:33.141607Z",
     "iopub.status.idle": "2022-03-02T00:13:33.364477Z",
     "shell.execute_reply": "2022-03-02T00:13:33.363801Z",
     "shell.execute_reply.started": "2022-03-02T00:13:33.142161Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ploting Loss Vs Val_Loss\n",
    "losses[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:13:35.454530Z",
     "iopub.status.busy": "2022-03-02T00:13:35.454230Z",
     "iopub.status.idle": "2022-03-02T00:13:35.687335Z",
     "shell.execute_reply": "2022-03-02T00:13:35.686512Z",
     "shell.execute_reply.started": "2022-03-02T00:13:35.454498Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting  Accutacy vs Val_accuracy\n",
    "losses[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successfully balances Model2 and Model3 by reducing the overfitting observed in Model2 with decreased steps per epoch, while also increasing accuracy with a greater number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:00:01.359379Z",
     "iopub.status.busy": "2022-03-02T00:00:01.359181Z",
     "iopub.status.idle": "2022-03-02T00:00:01.591232Z",
     "shell.execute_reply": "2022-03-02T00:00:01.590532Z",
     "shell.execute_reply.started": "2022-03-02T00:00:01.359354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "print('Test predictions...')\n",
    "ids = test.id\n",
    "preds = model4.predict(Xtest)\n",
    "\n",
    "# Submit\n",
    "submission = pd.DataFrame(preds,index=ids,columns=le.classes_)\n",
    "submission.to_csv('Leaf_Predictions_CNN_model4.csv')\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Prediction & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:03:49.051368Z",
     "iopub.status.busy": "2022-03-02T00:03:49.050578Z",
     "iopub.status.idle": "2022-03-02T00:03:49.057412Z",
     "shell.execute_reply": "2022-03-02T00:03:49.055922Z",
     "shell.execute_reply.started": "2022-03-02T00:03:49.051325Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain = train_images, target\n",
    "#Xval, yval = Xtrain, ytrain # Bigger validation set\n",
    "print('\\nXtrain, ytrain shapes ' + str((Xtrain.shape, ytrain.shape)))\n",
    "#print('Xval, yval shapes ' + str((Xval.shape, yval.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:03:49.059818Z",
     "iopub.status.busy": "2022-03-02T00:03:49.058918Z",
     "iopub.status.idle": "2022-03-02T00:03:49.071381Z",
     "shell.execute_reply": "2022-03-02T00:03:49.070585Z",
     "shell.execute_reply.started": "2022-03-02T00:03:49.059775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape data as a 4-dim tensor [number samples, width, height, color channels]\n",
    "print('Reshape as 4-dim tensor (Tensorflow backend)')\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0], img_rows, img_cols, 1) # 792\n",
    "#Xval = Xval.reshape(Xval.shape[0], img_rows, img_cols, 1) # 198\n",
    "Xtest = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "#print(Xtrain.shape, Xval.shape, Xtest.shape)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:03:49.074046Z",
     "iopub.status.busy": "2022-03-02T00:03:49.073472Z",
     "iopub.status.idle": "2022-03-02T00:03:49.082584Z",
     "shell.execute_reply": "2022-03-02T00:03:49.081697Z",
     "shell.execute_reply.started": "2022-03-02T00:03:49.074009Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "print('Train convolutional neural network')\n",
    "# Parameters\n",
    "# Batch size: number of training examples in one forward/backward pass. \n",
    "# The higher the batch size, the more memory space you'll need\n",
    "batch_size = 32\n",
    "# Epoch: one forward pass and one backward pass of all the training examples\n",
    "nb_epoch = 15\n",
    "n_extension = 10\n",
    "#samples_per_epoch = batch_size*(n_extension*Xtrain.shape[0] // batch_size)\n",
    "samples_per_epoch = n_extension*Xtrain.shape[0] // batch_size\n",
    "\n",
    "# number of convolutional filters to use (a hidden layer is segmented into feature maps\n",
    "# where each unit in a feature map looks for the same feature but at different positions of the input image)\n",
    "#nb_filters = 64\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "# Number of classes\n",
    "nb_classes = np.unique(ytrain).size\n",
    "#kernel_size = (5, 5)\n",
    "print('Model Parameters:')\n",
    "print('Batch size: %d, epochs: %d, samples per epoch: %d, n classes: %d' % \n",
    "                                (batch_size, nb_epoch,samples_per_epoch, nb_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:03:49.084886Z",
     "iopub.status.busy": "2022-03-02T00:03:49.083970Z",
     "iopub.status.idle": "2022-03-02T00:03:49.153526Z",
     "shell.execute_reply": "2022-03-02T00:03:49.152845Z",
     "shell.execute_reply.started": "2022-03-02T00:03:49.084848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices (one-hot encoder)\n",
    "ytrain = np_utils.to_categorical(ytrain, nb_classes)\n",
    "yval = np_utils.to_categorical(yval, nb_classes)\n",
    "\n",
    "\n",
    "# Create model\n",
    "final_model = Sequential()\n",
    "\n",
    "# Add hidden layers\n",
    "\n",
    "# Conv2D layer with 5x5 kernels (local weights) and 16 conv filters \n",
    "# (or feature maps), expects 2d images as inputs\n",
    "final_model.add(Conv2D(filters=16, kernel_size=(5,5),  padding='same', input_shape=input_shape,activation='relu',))\n",
    "final_model.add(MaxPooling2D(pool_size=pool_size))\n",
    "final_model.add(Dropout(0.5)) # Regularization method, exclude 50% units\n",
    "\n",
    "# Stack two 3x3 conv2D layers (Instead of one 5x5)\n",
    "final_model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=input_shape,activation='relu',))\n",
    "final_model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=input_shape,activation='relu',))\n",
    "\n",
    "# Pool2D layer, a form of non-linear down-sampling to prevent\n",
    "# overfitting and provide a form of translation invariance\n",
    "final_model.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Flattenig layer, converts 2D matrix into vectors\n",
    "final_model.add(Flatten())\n",
    "\n",
    "# Standard fully connected layer with 128 units\n",
    "final_model.add(Dense(128))\n",
    "final_model.add(Activation('relu'))\n",
    "\n",
    "# Output layer\n",
    "final_model.add(Dense(nb_classes))\n",
    "final_model.add(Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "final_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:03:49.156937Z",
     "iopub.status.busy": "2022-03-02T00:03:49.156749Z",
     "iopub.status.idle": "2022-03-02T00:05:30.659810Z",
     "shell.execute_reply": "2022-03-02T00:05:30.659063Z",
     "shell.execute_reply.started": "2022-03-02T00:03:49.156914Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('\\nTraining Keras Convolutional Neural Network...')\n",
    "# Fit model with generator\n",
    "results = final_model.fit_generator(imageGenerator(Xtrain, ytrain, batch_size), \n",
    "                    steps_per_epoch = samples_per_epoch,\n",
    "                    epochs=nb_epoch, verbose=1)\n",
    "#model.fit(Xtrain, ytrain, batch_size=batch_size, epochs=nb_epoch,\n",
    "#          verbose=1, validation_data=(Xval, yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-02T00:06:04.286922Z",
     "iopub.status.busy": "2022-03-02T00:06:04.286137Z",
     "iopub.status.idle": "2022-03-02T00:06:04.507187Z",
     "shell.execute_reply": "2022-03-02T00:06:04.506481Z",
     "shell.execute_reply.started": "2022-03-02T00:06:04.286878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "print('Test predictions...')\n",
    "ids = test.id\n",
    "preds = final_model.predict(Xtest)\n",
    "\n",
    "# Submit\n",
    "submission = pd.DataFrame(preds,index=ids,columns=le.classes_)\n",
    "submission.to_csv('Leaf_Predictions_CNN.csv')\n",
    "submission.head()"
   ]
  },
  {
   "attachments": {
    "0fa39886-3dd4-4bdb-8946-0b7b07fe35bb.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![final_model.png](attachment:0fa39886-3dd4-4bdb-8946-0b7b07fe35bb.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
